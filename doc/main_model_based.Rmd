---
title: "Project 3 - Main script"
author: "Alek Anichowski, Jerome Kafrouni, Pak Kin Lai, Chunzi Wang"
date: "March 5, 2018"
output:
  pdf_document: default
  html_document: default
---

# Load libraries

```{r}
+source("../lib/functions.R")
+source("../lib/em.R")
+source("../lib/model_based_predictions.R")
+source("../lib/functions.R")
+source("../lib/cross_validation.R")
```

# Load and preprocess the data

```{r}
# movie_train <- read.csv("../data/eachmovie_sample/data_train.csv", as.is = TRUE, header = TRUE)
# MS_train <- read.csv("../data/MS_sample/data_train.csv", as.is = TRUE, header = TRUE)
```

We first convert the datasets into user-item (UI) matrices. In these matrices, position (i,j) contains the rating of user i for item j.

```{r}
# system.time(MS_UI <- MS_data_transform(MS_train))
# system.time(movies_UI <- movie_data_transform(movie_train))

# save(MS_UI, file = "../output/ms_UI.RData")
# save(movies_UI, file = "../output/movies_UI.RData")
```

For convenience we have already computed the UI matrices (the code above takes a few minutes to run):

```{r}
load("../output/MS_UI.RData")
load("../output/movie_UI.RData")
```


```{r}
dim(movie_UI)
dim(MS_UI)
```

# Task 1: implementing two models for Collaborative Filtering

## Memory-based approach

See other main file, *main_memory_based.rmd*.

## Model-based approach

The main idea is to model the ratings of users by assigning them to latent (uknown) clusters. In each cluster, we make the assumption that users rate movies similarly. We also make a Naive Bayes assumption over all the movies each user rates.

Therefore, we can think of the rating process as the following sampling process: for a user, first we sample its cluster assignment, then we sample the rating of a given movie which follows the same distribution for all users of the cluster (in this project, we assume that this distribution is multinomial.).

Therefore we need to approximate a set of parameters which are the probabilities of being in each class, and the probabilities of giving each rating to each movie for each class. If we knew the cluster assignments, we could use a simple Maximum Likelihood Estimator of the parameters for each class. Since the cluster assignments are actually unknown, we have to use the EM algorithm.

The EM algorithm is very similar to k-means: it's done in two steps, the first is to assign points to clusters (expectation or "E" step), and the second is to update the parameters for each cluster (maximization or "M" step). The key difference to k-means is that here the cluster assignments are "soft" assignment i.e. each point has a given probability of being in each cluster.

To choose the number of clusters (which is an input of the EM algorithm), we use Cross Validation. There are more complex techniques that can be used for this task, yet not used nor implemented for this project.

### Step 1: Run the EM algorithm to estimate parameters:

- *assignment* corresponds to the M matrix on the tutorial's slides (or the a_i_c values)
- *prob_class* corresponds to the mu_c values (probability of each class)
- *prob_rating* corresponds to the gamma_c_j_k values (prob_rating[class c, movie j, rating k])

#### Select number of clusters by cross-validation:

Movies:

```{r}
model_values <- list()
for (C in list(3, 5, 8, 10, 15, 20)){
  model_values[[length(model_values)+1]] <- list(C=C, epsilon=1)
}

err_cv <- array(dim = c(length(model_values), 2))
for(k in 1:length(model_values)){
    cat("k=", k, "\n")
    err_cv[k,] <- cv.function(movie_UI, K=3, par=model_values[[k]], dataset='movies')
    print(paste0('err for this k: ', err_cv[k,]))
}

print(paste0('best param for movies: ', model_values[which.min(err_cv[,1])]))
print(paste0('best accuracy for movies: ', min(err_cv[,1])))
```

MS dataset:

```{r}
model_values <- list()
for (C in list(3, 5, 8, 10, 15, 20)){
  model_values[[length(model_values)+1]] <- list(C=C, epsilon=1)
}

err_cv <- array(dim = c(length(model_values), 2))
for(k in 1:length(model_values)){
    cat("k=", k, "\n")
    err_cv[k,] <- cv.function(MS_UI, K=3, par=model_values[[k]], dataset='MS')
    print(paste0('err for this k: ', err_cv[k,]))
}

print(paste0('best param for MS: ', model_values[which.max(err_cv[,1])]))
print(paste0('best accuracy for MS: ', max(err_cv[,1])))
```

#### Train on full training data

Now that we have selected the best number of clusters (C=8 for both datasets), we train again on the whole dataset. This time, we check for convergence with a smaller epsilon to increase accuracy since we'll be using these parameters for the actual predictions (in cross-validation we could afford a lower precision).

```{r}
system.time(result <- em(movie_UI, C=8, epsilon=0.01, dataset='movies'))

assignment_movies <- result[[1]]
prob_class_movies <- result[[2]]
prob_rating_movies <- result[[3]]

system.time(result <- em(MS_UI, C=8, epsilon=0.01, dataset='MS'))

assignment_MS <- result[[1]]
prob_class_MS <- result[[2]]
prob_rating_MS <- result[[3]]
```

### Step 2: Check results:

Check if probabilities are correct:

```{r}
em_check(assignment_movies, prob_class_movies, prob_rating_movies)
# There is only one issue with one row of movie_UI which seems to be coming from the dataset.
em_check(assignment_MS, prob_class_MS, prob_rating_MS)
```

### Step 3: Predict ratings

Now that we have approximated parameters, we can use them to get expected ratings of items. The formulas are close to what we used during the EM algorithm. We fill in all the NAs from the UI matrix with expected ratings. Here, we do pass the UI matrix as an argument of the predict function, since we're going to fill in this matrix, but note that in general we do not need this UI matrix (if we were doing predictions for other cases), which is very useful: we don't have to keep the UI matrix in memory after approximating the parameters, which is a main advantage of this approach (compared to memory-based approach). It's similar to the difference between models such as k-nearest-neighbors (where we have to keep all points of the training data to compute predictions) verus other ML algorithms.

Note: *model_based_predictions.R* contains depreceated (because slow) version of the prediction functions that use for loops instead of matrix computations. Here we use a faster version (*predict_all_v2*) to compute the predictions:

```{r}
system.time(predicted_movies_UI <- predict_all_v2(movie_UI, prob_class_movies, prob_rating_movies, assignment_movies))
system.time(predicted_MS_UI <- predict_all_v2(MS_UI, prob_class_MS, prob_rating_MS, assignment_MS, dataset='MS'))
```

### Step 4: Measure performance on test data

The test data contains some ratings that were missing for users. We can compare our predictions with these values.

```{r}
# movie_test <- read.csv("../data/eachmovie_sample/data_test.csv", as.is = TRUE, header = TRUE)
# MS_test <- read.csv("../data/MS_sample/data_test.csv", as.is = TRUE, header = TRUE)
```

```{r}
# system.time(MS_UI_test <- MS_data_transform(MS_test))
# system.time(movie_UI_test <- movie_data_transform(movie_test))

# save(MS_UI_test, file = "../output/MS_UI_test.RData")
# save(movie_UI_test, file = "../output/movie_UI_test.RData")
```

```{r}
load("../output/MS_UI_test.RData")
load("../output/movie_UI_test.RData")
```

```{r}
test_acc_MS(predicted_MS_UI, MS_UI_test)
test_acc_movie(predicted_movies_UI, movie_UI_test)
```
